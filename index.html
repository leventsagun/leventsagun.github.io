<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Levent Sagun</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <div class="page">
    <header>
      <h1>Levent Sagun</h1>
      <p class="subtitle">
        Research Scientist, FAIR at Meta, Paris
      </p>
      <nav>
        <a href="#about">About</a>
        <a href="#research">Research</a>
        <a href="#publications">Selected work</a>
        <a href="#teaching">Teaching</a>
        <a href="#contact">Contact</a>
        <a href="things/CV-LeventSagun.pdf">CV (PDF)</a>
      </nav>
    </header>

    <section id="about">
      <h2>About</h2>
      <p>
        I am a Research Scientist at FAIR in Paris. I study
        failure modes in large models, with a focus on contextualized
        measurement for AI governance: construct validity, brittleness,
        spurious correlations, and fairness under distribution shift.
      </p>
      <p>
        Previously, I was a postdoctoral fellow at EPFL and ENS Paris
        as part of the Simons Collaboration on Cracking the Glass Problem.
        I received my Ph.D. in Mathematics from the Courant Institute of
        Mathematical Sciences at NYU.
      </p>
    </section>

    <section id="research">
      <h2>Research interests</h2>
      <ul>
        <li>Contextualized measurement for AI governance and construct validity</li>
        <li>Spurious correlations, bias amplification, and fairness under shift</li>
        <li>Robustness and reliability of large models in real-world settings</li>
        <li>Optimization, over-parameterization, and loss landscapes in deep learning</li>
      </ul>
    </section>

    <section id="publications">
      <h2>Selected recent work</h2>
      <ul>
        <li>
          <strong>LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance.</strong><br />
          P. Haller, M. Ibrahim, P. Kirichenko, L. Sagun, S. Bell.
          <em>arXiv 2025.</em>
        </li>
        <li>
          <strong>Issues in Measuring the Fairness of Social Representation in Synthetic (Speech) Data.</strong><br />
          A. Subramonian, B. Sheppard, L. Sagun.
          <em>Synthetic Data Workshop at Aarhus Decennial Conference 2025.</em>
        </li>
        <li>
            <strong>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models.</strong><br />
            C. Shaib, V. Suriyakumar, L. Sagun, B. Wallace, M. Ghassemi.
            <em>Spotlight at NeurIPS 2025.</em>
        </li>
        <li>
          <strong>On the lack of queer voices in diverse speech datasets.</strong><br />
          B. Sheppard, E. Ovalle, A. Williams, L. Sagun.
          <em>Social Science and Language Models workshop at Weizenbaum Institute 2025 &amp; Speech AI for All Workshop at CHI 2025.</em>
        </li>
        <li>
          <strong>The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models.</strong><br />
          A. Ovalle, K. L. Pavasovic, L. Martin, L. Zettlemoyer, E. M. Smith, K.-W. Chang, A. Williams, L. Sagun.
          <em>Queer in AI at NeurIPS 2024 &amp; FAccT 2025.</em>
        </li>
        <li>
          <strong>An Effective Theory of Bias Amplification.</strong><br />
          A. Subramonian, S. J. Bell, L. Sagun, E. Dohmatob.
          <em>ICLR 2025.</em>
        </li>
        <li>
          <strong>A Differentiable Rank-Based Objective For Better Feature Learning.</strong><br />
          K. Lehman Pavasovic, D. Lopez-Paz, G. Biroli, L. Sagun.
          <em>ICLR 2025.</em>
        </li>
        <li>
          <strong>On generated vs collected data.</strong><br />
          L. Sagun, K. Ahuja, E. Dohmatob, J. Kempe.
          <em>The Workshop on Global AI Cultures at ICLR, 2024.</em>
        </li>
        <li>
          <strong>Simplicity bias leads to amplified performance disparities.</strong><br />
          S. Bell, L. Sagun.
          <em>FAccT 2023.</em>
        </li>
        <li>
          <strong>Fairness Indicators for Systematic Assessments of Visual Feature Extractors.</strong><br />
          P. Goyal, A. R. Soriano, C. Hazirbas, L. Sagun, N. Usunier.
          <em>FAccT 2022.</em>
        </li>
      </ul>
      <p>
        For a full and up-to-date list of publications, see
        <a href="https://scholar.google.com/citations?user=-iPZaBcAAAAJ&hl=en">Google Scholar</a>.
      </p>
    </section>

    <section id="teaching">
      <h2>Teaching</h2>
      <p>
        I have taught and assisted courses in probability, statistics, machine
        learning, and data science at NYUâ€™s Courant Institute and Center for
        Data Science, and have given invited lectures and short courses on deep
        learning and AI theory.
      </p>
    </section>

    <section id="contact" class="contact">
      <h2>Contact</h2>
      <p>Email: leventsagun@{gmail or meta}.com </p>
      <p>Google Scholar:
        <a href="https://scholar.google.com/citations?user=-iPZaBcAAAAJ&hl=en">
          Profile
        </a>
      </p>
      <p>GitHub:
        <a href="https://github.com/leventsagun">github.com/leventsagun</a>
      </p>
    </section>

    <footer>
      <p>Last updated: November 2025.</p>
    </footer>
  </div>
</body>
</html>
